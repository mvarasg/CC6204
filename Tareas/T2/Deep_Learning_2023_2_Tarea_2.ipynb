{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **[Pon tu nombre aquí]**"
      ],
      "metadata": {
        "id": "vsUOxBVoYfa0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oVa9eAqJIvv"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "En este notebook resolveremos un problema real usando redes pre-entrenadas y transfer learning.\n",
        "\n",
        "## Patrones geométricos de Kunisch\n",
        "En el año 1998, Norbert Kunisch (un reconocido arqueólogo clásico alemán) publicó su libro \"Ornamente Geometrischer Vasen\", en donde clasificó diferentes patrones geométricos de vasijas griegas que se encuentran en diferentes museos de Europa. Su trabajo fue muy importante para determinar tiempos y ubicaciones de muchos objetos esparcidos por todo el continente.\n",
        "\n",
        "Su libro es un compendio de los diferentes patrones geométricos presentes en vasijas y las anotaciones textuales de los patrones en diferentes idiomas.\n",
        "\n",
        "<img src=\"http://www.ivan-sipiran.com/downloads/0.jpg\" alt=\"Test\" height=\"200\" />\n",
        "\n",
        "<img src=\"http://www.ivan-sipiran.com/downloads/6.jpg\" alt=\"Test\" height=\"200\" />\n",
        "\n",
        "<img src=\"http://www.ivan-sipiran.com/downloads/36.jpg\" alt=\"Test\" height=\"200\" />\n",
        "\n",
        "<img src=\"http://www.ivan-sipiran.com/downloads/92.jpg\" alt=\"Test\" height=\"200\" />\n",
        "\n",
        "<img src=\"http://www.ivan-sipiran.com/downloads/210.jpg\" alt=\"Test\" height=\"200\" />\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izv2Fl1vQpQY"
      },
      "source": [
        "# El problema\n",
        "\n",
        "En el museo de arqueología Schloss Eggenberg quieren realizar un análisis automático de patrones en vasijas usando el compendio de Kunisch. Para esto nosotros realizamos primero un trabajo de escaneo del libro entero y de OCR para extraer tanto los patrones como las anotaciones textuales.\n",
        "\n",
        "Como resultado tenemos 348 patrones, clasificados en seis clases:\n",
        "\n",
        "*   Ornamentos circulares\n",
        "*   Ornamentos triangulares\n",
        "*   Ornamenros rectangulares\n",
        "*   Pictografías\n",
        "*   Trazos y líneas\n",
        "*   Romboidales\n",
        "\n",
        "Nuestro primer objetivo es encontrar una forma de lograr una alta eficacia de clasificación de estos patrones. Sin embargo, al tener muy poca data, trataremos de resolverlo con la técnica dle transfer learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Descargamos la data\n",
        "!wget http://www.ivan-sipiran.com/downloads/data_patterns2.zip\n",
        "!unzip data_patterns2.zip"
      ],
      "metadata": {
        "id": "cdL_5emO77kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F7bCYJeSSwA"
      },
      "source": [
        "# La Solución\n",
        "\n",
        "Primero creamos nuestras tres colecciones de datos: train, val y test. Los datos de train serán usados para entrenar el modelo, los datos de validación serán usados para probar el modelo durante entrenamiento, y los datos de test serán usados para evaluar el performance final del modelo.\n",
        "\n",
        "Hacemos un poco de data engineering para crear nuestras colecciones.\n",
        "\n",
        "*   Datos de train: 70%\n",
        "*   Datos de val: 10%\n",
        "*   Datos de test: 20%\n",
        "\n",
        "La partición se hace dentro de cada clase para asegurar representatividad en cada clase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSX8rT0zwSn2"
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "random.seed(30)\n",
        "\n",
        "#Leemos la metadata del dataset desde el archivo CSV\n",
        "\n",
        "df = pd.read_csv('data/class_labels.csv', header=None)\n",
        "classes = df[1].unique()\n",
        "classesFinal = [cl.replace(' ', '_') for cl in classes]\n",
        "print(classesFinal)\n",
        "\n",
        "#Creamos folders para almacenar la data. NOTA: trata de ejecutar esta celda solo una vez, ya que\n",
        "# crea los folders y ese paso solo se ejecuta una vez\n",
        "try:\n",
        "  os.mkdir('dataset')\n",
        "except OSError:\n",
        "  print (\"No se pudo crear folder dataset\")\n",
        "else:\n",
        "  print (\"Se creó folder dataset\")\n",
        "\n",
        "try:\n",
        "  os.mkdir('dataset/train')\n",
        "except OSError:\n",
        "  print (\"No se pudo crear folder dataset\")\n",
        "else:\n",
        "  print (\"Se creó folder dataset\")\n",
        "\n",
        "try:\n",
        "  os.mkdir('dataset/test')\n",
        "except OSError:\n",
        "  print (\"No se pudo crear folder dataset\")\n",
        "else:\n",
        "  print (\"Se creó folder dataset\")\n",
        "\n",
        "try:\n",
        "  os.mkdir('dataset/val')\n",
        "except OSError:\n",
        "  print (\"No se pudo crear folder dataset\")\n",
        "else:\n",
        "  print (\"Se creó folder dataset\")\n",
        "\n",
        "for cl in classesFinal:\n",
        "  try:\n",
        "    os.mkdir(os.path.join('dataset', \"train\", cl))\n",
        "  except OSError:\n",
        "    print (f\"No se pudo crear folder train {cl}\")\n",
        "  else:\n",
        "    print (f\"Se creó folder train {cl}\")\n",
        "\n",
        "  try:\n",
        "    os.mkdir(os.path.join('dataset', \"test\", cl))\n",
        "  except OSError:\n",
        "    print (f\"No se pudo crear folder test {cl}\")\n",
        "  else:\n",
        "    print (f\"Se creó folder test {cl}\")\n",
        "\n",
        "  try:\n",
        "    os.mkdir(os.path.join('dataset', \"val\", cl))\n",
        "  except OSError:\n",
        "    print (f\"No se pudo crear folder val {cl}\")\n",
        "  else:\n",
        "    print (f\"Se creó folder val {cl}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1H4hT2SCU8B"
      },
      "source": [
        "#Hacemos el pre-procesamiento de los datos, cada imagen va a parar a su respectivo folder\n",
        "dataset = dict()\n",
        "\n",
        "for index, cl in enumerate(classes):\n",
        "    grouped_data = df.groupby(1).get_group(cl)[0].tolist()\n",
        "    dataset[classesFinal[index]] = grouped_data\n",
        "\n",
        "for k,v in dataset.items():\n",
        "    print(f'Class: {k}, Length: {len(v)}')\n",
        "\n",
        "for k,v in dataset.items():\n",
        "    valNumber = math.ceil(0.1 * len(v))\n",
        "    testNumber = math.ceil(0.2 * len(v))\n",
        "    trainNumber = len(v) - valNumber - testNumber\n",
        "\n",
        "    random.shuffle(v)\n",
        "    elemTrain = v[:trainNumber]\n",
        "    elemVal = v[trainNumber:trainNumber+valNumber]\n",
        "    elemTest = v[trainNumber+valNumber:]\n",
        "\n",
        "    assert (valNumber + testNumber + trainNumber) == len(v)\n",
        "\n",
        "    pathTrain = './dataset/train/'\n",
        "    pathSource = './data/patrones/'\n",
        "\n",
        "    #Copiar archivos de train\n",
        "    for elem in elemTrain:\n",
        "        shutil.copy(os.path.join(pathSource,elem,elem+'_pattern.png'), os.path.join(pathTrain, k))\n",
        "\n",
        "    pathTest = './dataset/test/'\n",
        "    pathSource = './data/patrones/'\n",
        "\n",
        "    #Copiar archivos de test\n",
        "    for elem in elemTest:\n",
        "        shutil.copy(os.path.join(pathSource,elem,elem+'_pattern.png'), os.path.join(pathTest, k))\n",
        "\n",
        "    pathVal = './dataset/val/'\n",
        "    pathSource = './data/patrones/'\n",
        "\n",
        "    #Copiar archivos de train\n",
        "    for elem in elemVal:\n",
        "        shutil.copy(os.path.join(pathSource,elem,elem+'_pattern.png'), os.path.join(pathVal, k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQUPRSH5WBTN"
      },
      "source": [
        "#Creación de Datasets\n",
        "\n",
        "Se crean los datasets y dataloaders de Pytorch, que usarás para tu tarea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8BItV-vWFaX"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "pathDataset = 'dataset/'\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(pathDataset + 'train',\n",
        "                                                    transform = transforms.Compose([\n",
        "                                                        transforms.RandomVerticalFlip(),\n",
        "                                                        transforms.RandomHorizontalFlip(),\n",
        "                                                        transforms.RandomResizedCrop(224),\n",
        "                                                                    transforms.ToTensor(),\n",
        "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
        "\n",
        "val_dataset = torchvision.datasets.ImageFolder(pathDataset + 'val',\n",
        "                                                    transform = transforms.Compose([ transforms.Resize(256),\n",
        "                                                                    transforms.CenterCrop(224),\n",
        "                                                                    transforms.ToTensor(),\n",
        "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
        "\n",
        "test_dataset = torchvision.datasets.ImageFolder(pathDataset + 'test',\n",
        "                                                    transform = transforms.Compose([ transforms.Resize(224),\n",
        "                                                                    transforms.ToTensor(),\n",
        "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qtajXrKVoJF"
      },
      "source": [
        "# Solución 1: Fine-tuning\n",
        "\n",
        "Utiliza fine-tuning para el problema anterior y reporta el accuracy de test. Intenta obtener el accuracy de test más alto posible, sin que el modelo haga overfitting. Considera overfitting cuando la diferencia entre accuracy de train y accuracy de validación tengan una diferencia de más de 5%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCyaAfKQWk4M"
      },
      "source": [
        "# Solución 2: Freezing\n",
        "\n",
        "Utiliza freezing para el problema anterior y reporta el accuracy de test. Intenta obtener el accuracy de test más alto posible, sin que el modelo haga overfitting. Considera overfitting\n",
        "ting cuando la diferencia entre accuracy de train y accuracy de validación tengan una diferencia de más de 5%."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7B8uc-Mx90Nc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
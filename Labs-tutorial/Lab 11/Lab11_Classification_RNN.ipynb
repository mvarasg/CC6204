{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "v42xWF71CGSp",
        "yD3COp6YECo4",
        "hcx0QqRgIUrq",
        "NzFm6PGJI1So",
        "LxEk6_qLKnZD",
        "96r_MDRgN6eS",
        "caV4lvLXOMsY",
        "C8oTHpbxOcoA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Crearemos una red neuronal recurrente para clasificar reviews de películas. Las clases son \"positivo\" o \"negativo\". \n",
        "Primero descargamos los datos"
      ],
      "metadata": {
        "id": "l3HU_r_9BL2d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLr8whGj-wCZ"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "!wget -c https://www.ivan-sipiran.com/downloads/labels.txt\n",
        "!wget -c https://www.ivan-sipiran.com/downloads/reviews.txt\n",
        "!mv *.txt data/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargamos los archivos de texto\n",
        "with open('data/reviews.txt', 'r') as f:\n",
        "  reviews = f.read()\n",
        "with open('data/labels.txt', 'r') as f:\n",
        "  labels = f.read()\n",
        "\n",
        "#reviews y labels como un solo string que almacena todo el archivo"
      ],
      "metadata": {
        "id": "nvzT3lz-BbRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviews[:200])\n",
        "print()\n",
        "print(labels[:20])"
      ],
      "metadata": {
        "id": "smFhI8QQB0-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-procesamiento de datos\n",
        "\n",
        "Necesitamos tener los datos en una forma que haga posible el uso de una red nueronal para procesarlo. En este laboratorio, usaremos una representación a nivel de palabras, por lo que necesitamos una forma de representar cada palabra cde forma numérica. Aquí aprenderemos cómo procesar el texto y convertirlo en una representación idónea.\n",
        "\n",
        "Para simplificar el trabajo, tendremos en cuenta:\n",
        "\n",
        "*   No nos interesan los signos de puntuación\n",
        "*   Dividir los reviews (un solo string) en strings individuales para cada review. Usaremos el `\\n` como delimitador.\n",
        "\n",
        "Primero, eliminamos los signos de puntuación y dividimos el texto en palabras indiduales\n",
        "\n"
      ],
      "metadata": {
        "id": "v42xWF71CGSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "print(punctuation)\n",
        "\n",
        "reviews = reviews.lower()\n",
        "\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])"
      ],
      "metadata": {
        "id": "87bm9QOaCDmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removemos los saltos de línea, y juntamos todo el texto de nuevo\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ''.join(reviews_split)\n",
        "\n",
        "words = all_text.split()\n",
        "print(words[:30])"
      ],
      "metadata": {
        "id": "dTOPXGHkDWVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codificando las palabras\n",
        "\n",
        "Vamos a representar cada palabra con un único número entero que representará su índice. Para eso construiremos un diccionario que mapee palabras a números."
      ],
      "metadata": {
        "id": "yD3COp6YECo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "counts = Counter(words) #Construye un diccionario de palabras. Las claves son las palabras y los valores son la frecuencia\n",
        "vocab = sorted(counts, key=counts.get, reverse=True) #Ordenamos la palabras por frecuencia\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)} #Construimos diccionario para mapear palabra a número entero. Empezamos los índices en 1\n",
        "\n",
        "#Ahora convertimos cada palabra de los reviews en índices\n",
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "  reviews_ints.append([vocab_to_int[word] for word in review.split()])\n"
      ],
      "metadata": {
        "id": "fUNyXic8DmyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cada review ahora se representa como una secuencia de números (índices)\n",
        "print(reviews_split[0])\n",
        "print(reviews_ints[0])"
      ],
      "metadata": {
        "id": "P0pND9oiE2ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cuántas palabras hay en el diccionario?\n",
        "print('Palabras únicas:', len(vocab_to_int))\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "id": "r3wNZ7YaE32Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding de etiquetas\n",
        "Las etiquetas se convierten a números para poder ser ingresadas a la red neuronal.\n"
      ],
      "metadata": {
        "id": "hcx0QqRgIUrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
      ],
      "metadata": {
        "id": "ALMjVSMxISGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Longitud de Secuencias\n",
        "Los reviews tienen distinto tamaño. Para poder procesar estos datos de manera efectiva en una red neuronal, tenemos que uniformizar los tamaños. Esto permite la creación de batches para el entrenamiento."
      ],
      "metadata": {
        "id": "NzFm6PGJI1So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sacamos algunas estadísticas de los datos\n",
        "review_lens = Counter([len(x) for x in reviews_ints]) #Contamos cuantas palabras hay en cada review\n",
        "print(\"Reviews de longitud cero:\", review_lens[0])\n",
        "print('Máxima longitud:', max(review_lens))"
      ],
      "metadata": {
        "id": "LuZ-S1JwIsQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay reviews vacíos y reviews muy grandes para ser procesados por una RNN. Primero eliminamos los reviews vacíos y truncamos los reviews muy grandes"
      ],
      "metadata": {
        "id": "EbtpqglKJppO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reviews antes de eliminación:', len(reviews_ints))\n",
        "\n",
        "#Extraemos los índices de todos los reviews que tienen longitud > 0\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review)!=0]\n",
        "\n",
        "#Nos quedamos solo con los reviews con longitud > 0\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "\n",
        "#Lo mismo con los labels\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Reviews después de eliminación:', len(reviews_ints))"
      ],
      "metadata": {
        "id": "LKf0xi3FJTpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding\n",
        "La estrategia para uniformizar el tamaño de las secuencias es como sigue:\n",
        "\n",
        "\n",
        "*   Definir un tamaño para las secuencias: `seq_length`\n",
        "*   Reviews con más de `seq_length` palabras, truncamos el review a las primeras `seq_length` palabras.\n",
        "*   Reviews con menos de `seq_length` palabras, aplicamos padding con 0's al inicio de la secuencia. Por ejemplo el review `['best', 'movie', 'ever']`, `[117, 18, 128]` quedaría como `[0, 0 ,0, ...,117, 18, 128]`\n",
        "\n",
        "Como cada review ahora tendrá la misma longitud podemos convertir los datos en un array 2D, más conveniente para el proceso posterior."
      ],
      "metadata": {
        "id": "LxEk6_qLKnZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "  features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "\n",
        "  #Para cada review, se coloca en la matriz\n",
        "  for i, row in enumerate(reviews_ints):\n",
        "    features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "  \n",
        "  return features"
      ],
      "metadata": {
        "id": "EurK6LTiKlNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Probamos el padding\n",
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "print(features.shape)\n",
        "print(features[:30,:10])"
      ],
      "metadata": {
        "id": "Y4mHBmSvM60I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Partición de los datos\n"
      ],
      "metadata": {
        "id": "96r_MDRgN6eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "split_idx = int(len(features)*0.8)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeatures:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "metadata": {
        "id": "jvdP-cDqNNWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Datasets y Dataloaders\n"
      ],
      "metadata": {
        "id": "caV4lvLXOMsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# crear Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "izkA1cS1OHHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creación de red neuronal\n",
        "Nuestra red tendrá los siguientes componentes:\n",
        "1. Una [capa embedding](https://pytorch.org/docs/stable/nn.html#embedding) que convierta tokens de palabras (índices) en embeddings de un tamaño específico.\n",
        "2. Una [capa LSTM](https://pytorch.org/docs/stable/nn.html#lstm) definida por el tamaño del estado oculto y el número de capas.\n",
        "3. Una capa de salida fully-connected que mapee la salida del LSTM al tamaño de salida deseado\n",
        "4. Una capa de activación sigmoide que convierta la salida a valores 0-1; debe retornar **solo la última salida sigmoide** como salida d ela red.\n",
        "\n",
        "### La capa embedding\n",
        "\n",
        "Necesitamos agregar una [capa embedding](https://pytorch.org/docs/stable/nn.html#embedding). Una opción podría ser usar one-hot encoding para el vocabulario, pero tenemos demasiadas palabras (~74,000). Así que mejor usamos una capa de embedding que sirva como tabla look-up para ujna determinada palabra. También se podría usar Word2Vec, pero ahora usamos una capa embedding que sea aprendida para este problema específico, usándolo como una forma de reducción de la dimensión del vocabulario original.\n",
        "\n",
        "### La capa LSTM\n",
        "\n",
        "Crearemos un [LSTM](https://pytorch.org/docs/stable/nn.html#lstm) como red recurrente, con los siguientes parámetros input_size, hidden_dim, el número de capas, la probabilidad de dropout (para dropout entre múltiples), y un parámetro batch_first.\n",
        "\n"
      ],
      "metadata": {
        "id": "C8oTHpbxOcoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chequear si tenemos GPU\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "metadata": {
        "id": "adKz6u-SOXuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos la red neuronal\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Capas embedding y LSTM\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        # Capa lineal y sigmoide\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "                \n",
        "        #Tomamos solo el último valor de salida del LSTM\n",
        "        lstm_out = lstm_out[:,-1,:]\n",
        "                \n",
        "        # dropout y fully-connected\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "               \n",
        "        # sigmoide\n",
        "        sig_out = self.sig(out)\n",
        "                  \n",
        "        # retornar sigmoide y último estado oculto\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Crea dos nuevos tensores con tamaño n_layers x batch_size x hidden_dim,\n",
        "        # inicializados a cero, para estado oculto y memoria de LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "metadata": {
        "id": "T3amSRw7QSHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciamos la red\n",
        "vocab_size = len(vocab_to_int) + 1 # +1 for zero padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400 \n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "metadata": {
        "id": "1IV3WtMJTYi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento\n",
        "\n",
        "Usaremos Binary Cross Entropy Loss, ideal para problemas binarios que usan sigmoide. Usamos gradient clipping también para evitar que los gradientes crezcan mucho."
      ],
      "metadata": {
        "id": "QG74D0RDfLNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "m-U2nRyYT2RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training params\n",
        "\n",
        "epochs = 4 \n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# Enviar red al GPU\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# Bucle de entrenamiento\n",
        "for e in range(epochs):\n",
        "    # Inicializar estado oculto\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # Bucle para batchs\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Crear nuevas variables para estados ocultos, sino se haría \n",
        "        # backprop para todos los pasos del bucle\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        net.zero_grad()\n",
        "\n",
        "        # Hacer pasada forward\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # Calcular loss y hacer backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # gradient clipping\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Mensajes\n",
        "        if counter % print_every == 0:\n",
        "            # Validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Época: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Paso: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "metadata": {
        "id": "ootGfDVmdR0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "Probamos de dos formas:\n",
        "\n",
        "\n",
        "*   Calcular accuracy de test\n",
        "*   Probar inferencia con reviews nuevos\n",
        "\n"
      ],
      "metadata": {
        "id": "x2uMK73bgIm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular accuracy de test\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# Iniciar estado oculto\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # Convertir probabilidades a clases (0,1)\n",
        "    pred = torch.round(output.squeeze())  \n",
        "    \n",
        "    # Comparar predicciones a labels\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# Accuracy de test\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "3uokiuQ5dZnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencia sobre reviews"
      ],
      "metadata": {
        "id": "o8Fd8Or6g2Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_review(test_review):\n",
        "    test_review = test_review.lower() \n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "    \n",
        "    test_words = test_text.split()\n",
        "    \n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "    \n",
        "    return test_ints"
      ],
      "metadata": {
        "id": "I6IRHh3deeWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, test_review, sequence_length=200):\n",
        "      \n",
        "    net.eval()\n",
        "    \n",
        "    test_ints = tokenize_review(test_review)\n",
        "    \n",
        "    seq_length = sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "      feature_tensor = feature_tensor.cuda()\n",
        "      \n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    pred = torch.round(output.squeeze())\n",
        "    print('Valor predicho, antes del redondeo: {:.6f}'.format(output.item()))\n",
        "    \n",
        "    # print custom response based on whether test_review is pos/neg\n",
        "    if(pred.item()==1):\n",
        "      print('Review positivo!')\n",
        "    else:\n",
        "      print('Review negativo!')"
      ],
      "metadata": {
        "id": "u5Jh6kEZehnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# negative test review\n",
        "test_review_neg = 'Honestly, the movie sucks.'\n",
        "\n",
        "# positive test review\n",
        "test_review_pos = 'Well done. This is the best movie I have ever watched so far.'"
      ],
      "metadata": {
        "id": "Qok9IMfGekyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seq_length=200\n",
        "predict(net, test_review_neg, seq_length)\n",
        "predict(net, test_review_pos, seq_length)"
      ],
      "metadata": {
        "id": "BOWZ-sRcenf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRV3pv6PepcS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
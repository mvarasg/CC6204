{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3HU_r_9BL2d"
      },
      "source": [
        "Crearemos una red neuronal recurrente para clasificar reviews de películas. Las clases son \"positivo\" o \"negativo\". \n",
        "Primero descargamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JLr8whGj-wCZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved under labels.txt\n",
            "\n",
            "Saved under reviews.txt\n",
            "c:\\Users\\mvarasg\\Documents\\Universidad\\CC6204 - Deep Learning\\Labs-tutorial\\Lab 11\\labels.txt\n",
            "c:\\Users\\mvarasg\\Documents\\Universidad\\CC6204 - Deep Learning\\Labs-tutorial\\Lab 11\\reviews.txt\n",
            "Se han movido         2 archivos.\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!python -m wget https://www.ivan-sipiran.com/downloads/labels.txt\n",
        "!python -m wget https://www.ivan-sipiran.com/downloads/reviews.txt\n",
        "!move *.txt data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nvzT3lz-BbRQ"
      },
      "outputs": [],
      "source": [
        "#Cargamos los archivos de texto\n",
        "with open('data/reviews.txt', 'r') as f:\n",
        "  reviews = f.read()\n",
        "with open('data/labels.txt', 'r') as f:\n",
        "  labels = f.read()\n",
        "\n",
        "#reviews y labels como un solo string que almacena todo el archivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "smFhI8QQB0-H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  \n",
            "\n",
            "positive\n",
            "negative\n",
            "po\n"
          ]
        }
      ],
      "source": [
        "print(reviews[:200])\n",
        "print()\n",
        "print(labels[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v42xWF71CGSp"
      },
      "source": [
        "# Pre-procesamiento de datos\n",
        "\n",
        "Necesitamos tener los datos en una forma que haga posible el uso de una red nueronal para procesarlo. En este laboratorio, usaremos una representación a nivel de palabras, por lo que necesitamos una forma de representar cada palabra cde forma numérica. Aquí aprenderemos cómo procesar el texto y convertirlo en una representación idónea.\n",
        "\n",
        "Para simplificar el trabajo, tendremos en cuenta:\n",
        "\n",
        "*   No nos interesan los signos de puntuación\n",
        "*   Dividir los reviews (un solo string) en strings individuales para cada review. Usaremos el `\\n` como delimitador.\n",
        "\n",
        "Primero, eliminamos los signos de puntuación y dividimos el texto en palabras indiduales\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "87bm9QOaCDmX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "from string import punctuation\n",
        "\n",
        "print(punctuation)\n",
        "\n",
        "reviews = reviews.lower()\n",
        "\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dTOPXGHkDWVu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me']\n"
          ]
        }
      ],
      "source": [
        "#Removemos los saltos de línea, y juntamos todo el texto de nuevo\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ''.join(reviews_split)\n",
        "\n",
        "words = all_text.split()\n",
        "print(words[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD3COp6YECo4"
      },
      "source": [
        "# Codificando las palabras\n",
        "\n",
        "Vamos a representar cada palabra con un único número entero que representará su índice. Para eso construiremos un diccionario que mapee palabras a números."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fUNyXic8DmyL"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "counts = Counter(words) #Construye un diccionario de palabras. Las claves son las palabras y los valores son la frecuencia\n",
        "vocab = sorted(counts, key=counts.get, reverse=True) #Ordenamos la palabras por frecuencia\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)} #Construimos diccionario para mapear palabra a número entero. Empezamos los índices en 1\n",
        "\n",
        "#Ahora convertimos cada palabra de los reviews en índices\n",
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "  reviews_ints.append([vocab_to_int[word] for word in review.split()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P0pND9oiE2ay"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   \n",
            "[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]\n"
          ]
        }
      ],
      "source": [
        "#Cada review ahora se representa como una secuencia de números (índices)\n",
        "print(reviews_split[0])\n",
        "print(reviews_ints[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "r3wNZ7YaE32Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras únicas: 74072\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Cuántas palabras hay en el diccionario?\n",
        "print('Palabras únicas:', len(vocab_to_int))\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcx0QqRgIUrq"
      },
      "source": [
        "#Embedding de etiquetas\n",
        "Las etiquetas se convierten a números para poder ser ingresadas a la red neuronal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ALMjVSMxISGg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzFm6PGJI1So"
      },
      "source": [
        "# Longitud de Secuencias\n",
        "Los reviews tienen distinto tamaño. Para poder procesar estos datos de manera efectiva en una red neuronal, tenemos que uniformizar los tamaños. Esto permite la creación de batches para el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LuZ-S1JwIsQe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reviews de longitud cero: 1\n",
            "Máxima longitud: 2514\n"
          ]
        }
      ],
      "source": [
        "#Sacamos algunas estadísticas de los datos\n",
        "review_lens = Counter([len(x) for x in reviews_ints]) #Contamos cuantas palabras hay en cada review\n",
        "print(\"Reviews de longitud cero:\", review_lens[0])\n",
        "print('Máxima longitud:', max(review_lens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbtpqglKJppO"
      },
      "source": [
        "Hay reviews vacíos y reviews muy grandes para ser procesados por una RNN. Primero eliminamos los reviews vacíos y truncamos los reviews muy grandes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LKf0xi3FJTpH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reviews antes de eliminación: 25001\n",
            "Reviews después de eliminación: 25000\n"
          ]
        }
      ],
      "source": [
        "print('Reviews antes de eliminación:', len(reviews_ints))\n",
        "\n",
        "#Extraemos los índices de todos los reviews que tienen longitud > 0\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review)!=0]\n",
        "\n",
        "#Nos quedamos solo con los reviews con longitud > 0\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "\n",
        "#Lo mismo con los labels\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Reviews después de eliminación:', len(reviews_ints))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxEk6_qLKnZD"
      },
      "source": [
        "# Padding\n",
        "La estrategia para uniformizar el tamaño de las secuencias es como sigue:\n",
        "\n",
        "\n",
        "*   Definir un tamaño para las secuencias: `seq_length`\n",
        "*   Reviews con más de `seq_length` palabras, truncamos el review a las primeras `seq_length` palabras.\n",
        "*   Reviews con menos de `seq_length` palabras, aplicamos padding con 0's al inicio de la secuencia. Por ejemplo el review `['best', 'movie', 'ever']`, `[117, 18, 128]` quedaría como `[0, 0 ,0, ...,117, 18, 128]`\n",
        "\n",
        "Como cada review ahora tendrá la misma longitud podemos convertir los datos en un array 2D, más conveniente para el proceso posterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EurK6LTiKlNL"
      },
      "outputs": [],
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "  features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "\n",
        "  #Para cada review, se coloca en la matriz\n",
        "  for i, row in enumerate(reviews_ints):\n",
        "    features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "  \n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Y4mHBmSvM60I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25000, 200)\n",
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
            " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   54    10    14   116    60   798   552    71   364     5]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   330   578    34     3   162   748  2731     9   325]\n",
            " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
            " [    1    20     6    76    40     6    58    81    95     5]\n",
            " [   54    10    84   329 26230 46427    63    10    14   614]\n",
            " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   40    26   109 17952  1422     9     1   327     4   125]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   10   499     1   307 10399    55    74     8    13    30]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ]
        }
      ],
      "source": [
        "#Probamos el padding\n",
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "print(features.shape)\n",
        "print(features[:30,:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96r_MDRgN6eS"
      },
      "source": [
        "#Partición de los datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jvdP-cDqNNWQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\t\tFeatures:\n",
            "Train set: \t\t(20000, 200) \n",
            "Validation set: \t(2500, 200) \n",
            "Test set: \t\t(2500, 200)\n"
          ]
        }
      ],
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "split_idx = int(len(features)*0.8)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeatures:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caV4lvLXOMsY"
      },
      "source": [
        "#Datasets y Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "izkA1cS1OHHF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# crear Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oTHpbxOcoA"
      },
      "source": [
        "#Creación de red neuronal\n",
        "Nuestra red tendrá los siguientes componentes:\n",
        "1. Una [capa embedding](https://pytorch.org/docs/stable/nn.html#embedding) que convierta tokens de palabras (índices) en embeddings de un tamaño específico.\n",
        "2. Una [capa LSTM](https://pytorch.org/docs/stable/nn.html#lstm) definida por el tamaño del estado oculto y el número de capas.\n",
        "3. Una capa de salida fully-connected que mapee la salida del LSTM al tamaño de salida deseado\n",
        "4. Una capa de activación sigmoide que convierta la salida a valores 0-1; debe retornar **solo la última salida sigmoide** como salida d ela red.\n",
        "\n",
        "### La capa embedding\n",
        "\n",
        "Necesitamos agregar una [capa embedding](https://pytorch.org/docs/stable/nn.html#embedding). Una opción podría ser usar one-hot encoding para el vocabulario, pero tenemos demasiadas palabras (~74,000). Así que mejor usamos una capa de embedding que sirva como tabla look-up para ujna determinada palabra. También se podría usar Word2Vec, pero ahora usamos una capa embedding que sea aprendida para este problema específico, usándolo como una forma de reducción de la dimensión del vocabulario original.\n",
        "\n",
        "### La capa LSTM\n",
        "\n",
        "Crearemos un [LSTM](https://pytorch.org/docs/stable/nn.html#lstm) como red recurrente, con los siguientes parámetros input_size, hidden_dim, el número de capas, la probabilidad de dropout (para dropout entre múltiples), y un parámetro batch_first.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "adKz6u-SOXuy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ]
        }
      ],
      "source": [
        "# Chequear si tenemos GPU\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "T3amSRw7QSHD"
      },
      "outputs": [],
      "source": [
        "#Creamos la red neuronal\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Capas embedding y LSTM\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        # Capa lineal y sigmoide\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "                \n",
        "        #Tomamos solo el último valor de salida del LSTM\n",
        "        lstm_out = lstm_out[:,-1,:]\n",
        "                \n",
        "        # dropout y fully-connected\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "               \n",
        "        # sigmoide\n",
        "        sig_out = self.sig(out)\n",
        "                  \n",
        "        # retornar sigmoide y último estado oculto\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Crea dos nuevos tensores con tamaño n_layers x batch_size x hidden_dim,\n",
        "        # inicializados a cero, para estado oculto y memoria de LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1IV3WtMJTYi5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Instanciamos la red\n",
        "vocab_size = len(vocab_to_int) + 1 # +1 for zero padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400 \n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG74D0RDfLNA"
      },
      "source": [
        "# Entrenamiento\n",
        "\n",
        "Usaremos Binary Cross Entropy Loss, ideal para problemas binarios que usan sigmoide. Usamos gradient clipping también para evitar que los gradientes crezcan mucho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "m-U2nRyYT2RV"
      },
      "outputs": [],
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ootGfDVmdR0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Época: 1/4... Paso: 100... Loss: 0.652338... Val Loss: 0.669781\n",
            "Época: 1/4... Paso: 200... Loss: 0.689331... Val Loss: 0.634450\n",
            "Época: 1/4... Paso: 300... Loss: 0.691844... Val Loss: 0.644297\n",
            "Época: 1/4... Paso: 400... Loss: 0.651023... Val Loss: 0.657756\n",
            "Época: 2/4... Paso: 500... Loss: 0.503450... Val Loss: 0.592749\n",
            "Época: 2/4... Paso: 600... Loss: 0.377793... Val Loss: 0.528883\n",
            "Época: 2/4... Paso: 700... Loss: 0.410100... Val Loss: 0.482730\n",
            "Época: 2/4... Paso: 800... Loss: 0.475427... Val Loss: 0.477227\n",
            "Época: 3/4... Paso: 900... Loss: 0.373290... Val Loss: 0.474435\n",
            "Época: 3/4... Paso: 1000... Loss: 0.498743... Val Loss: 0.477798\n",
            "Época: 3/4... Paso: 1100... Loss: 0.366799... Val Loss: 0.464988\n",
            "Época: 3/4... Paso: 1200... Loss: 0.240940... Val Loss: 0.452786\n",
            "Época: 4/4... Paso: 1300... Loss: 0.222352... Val Loss: 0.463868\n",
            "Época: 4/4... Paso: 1400... Loss: 0.185388... Val Loss: 0.539836\n",
            "Época: 4/4... Paso: 1500... Loss: 0.274854... Val Loss: 0.550065\n",
            "Época: 4/4... Paso: 1600... Loss: 0.276296... Val Loss: 0.487774\n"
          ]
        }
      ],
      "source": [
        "# training params\n",
        "\n",
        "epochs = 4 \n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# Enviar red al GPU\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# Bucle de entrenamiento\n",
        "for e in range(epochs):\n",
        "    # Inicializar estado oculto\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # Bucle para batchs\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Crear nuevas variables para estados ocultos, sino se haría \n",
        "        # backprop para todos los pasos del bucle\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        net.zero_grad()\n",
        "\n",
        "        # Hacer pasada forward\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # Calcular loss y hacer backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # gradient clipping\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Mensajes\n",
        "        if counter % print_every == 0:\n",
        "            # Validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Época: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Paso: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2uMK73bgIm7"
      },
      "source": [
        "# Testing\n",
        "\n",
        "Probamos de dos formas:\n",
        "\n",
        "\n",
        "*   Calcular accuracy de test\n",
        "*   Probar inferencia con reviews nuevos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3uokiuQ5dZnW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.458\n",
            "Test accuracy: 0.796\n"
          ]
        }
      ],
      "source": [
        "# Calcular accuracy de test\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# Iniciar estado oculto\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # Convertir probabilidades a clases (0,1)\n",
        "    pred = torch.round(output.squeeze())  \n",
        "    \n",
        "    # Comparar predicciones a labels\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# Accuracy de test\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Fd8Or6g2Fx"
      },
      "source": [
        "# Inferencia sobre reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "I6IRHh3deeWA"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_review(test_review):\n",
        "    test_review = test_review.lower() \n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "    \n",
        "    test_words = test_text.split()\n",
        "    \n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "    \n",
        "    return test_ints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "u5Jh6kEZehnx"
      },
      "outputs": [],
      "source": [
        "def predict(net, test_review, sequence_length=200):\n",
        "      \n",
        "    net.eval()\n",
        "    \n",
        "    test_ints = tokenize_review(test_review)\n",
        "    \n",
        "    seq_length = sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "      feature_tensor = feature_tensor.cuda()\n",
        "      \n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    pred = torch.round(output.squeeze())\n",
        "    print('Valor predicho, antes del redondeo: {:.6f}'.format(output.item()))\n",
        "    \n",
        "    # print custom response based on whether test_review is pos/neg\n",
        "    if(pred.item()==1):\n",
        "      print('Review positivo!')\n",
        "    else:\n",
        "      print('Review negativo!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Qok9IMfGekyT"
      },
      "outputs": [],
      "source": [
        "# negative test review\n",
        "test_review_neg = 'Honestly, the movie sucks.'\n",
        "\n",
        "# positive test review\n",
        "test_review_pos = 'Well done. This is the best movie I have ever watched so far.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BOWZ-sRcenf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valor predicho, antes del redondeo: 0.079916\n",
            "Review negativo!\n",
            "Valor predicho, antes del redondeo: 0.979711\n",
            "Review positivo!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "seq_length=200\n",
        "predict(net, test_review_neg, seq_length)\n",
        "predict(net, test_review_pos, seq_length)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v42xWF71CGSp",
        "yD3COp6YECo4",
        "hcx0QqRgIUrq",
        "NzFm6PGJI1So",
        "LxEk6_qLKnZD",
        "96r_MDRgN6eS",
        "caV4lvLXOMsY",
        "C8oTHpbxOcoA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

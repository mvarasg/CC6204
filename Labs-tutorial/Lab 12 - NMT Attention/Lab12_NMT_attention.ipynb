{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.ivan-sipiran.com/downloads/nmt_data.zip"
      ],
      "metadata": {
        "id": "hCHZkEaku-yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nmt_data.zip"
      ],
      "metadata": {
        "id": "W8neVVuavQUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!mv *.txt data/"
      ],
      "metadata": {
        "id": "ZtBzfXY6vZD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3hN6-LPnudm"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Tuple, Sequence, Any\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "import random\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "#Función que preprocesa las secuencias de entrada y retorna secuencias de índices numéricos\n",
        "def preprocess(\n",
        "    raw_src_sentence: List[str], #Secuencia source original\n",
        "    raw_trg_sentence: List[str], #Secuencia target original\n",
        "    src_word2idx: Dict[str, int], #Diccionario de idioma source\n",
        "    trg_word2idx: Dict[str, int], #Diccionario de idioma target\n",
        "    max_len: int #Máxima longitud de secuencia\n",
        ") -> Tuple[List[int], List[int]]: # Retorna dos listas que representan las secuencias, pero con números\n",
        "    \"\"\" Sentence preprocessor for Seq2Seq with Attention\n",
        "    Before training the model, you should preprocess the data to feed Seq2Seq.\n",
        "    Implement preprocessor with following rules.\n",
        "\n",
        "    Preprocess Rules:\n",
        "    1. All words should be converted into thier own index number by word2idx.\n",
        "    1-1. If there is no matched word in word2idx, you should replace that word by <UNK> token.\n",
        "    1-2. You have to use matched word2idx for each source/target language.\n",
        "\n",
        "    2. You have to insert <SOS>, <EOS> tokens properly into the target sentence.\n",
        "    2-1. You don't need them in source sentence.\n",
        "\n",
        "    3. The length of preprocessed sentences should not exceed max_len.\n",
        "    3-1. If the lenght of the sentence exceed max_len, you must truncate the back of the sentence.\n",
        "\n",
        "    Arguments:\n",
        "    raw_src_sentence -- raw source sentence without any modification\n",
        "    raw_trg_sentence -- raw target sentence without any modification \n",
        "    src_word2idx -- dictionary for source language which maps words to their unique numbers\n",
        "    trg_word2idx -- dictionary for target language which maps words to their unique numbers\n",
        "    max_len -- maximum length of sentences\n",
        "\n",
        "    Return:\n",
        "    src_sentence -- preprocessed source sentence\n",
        "    trg_sentence -- preprocessed target sentence\n",
        "\n",
        "    \"\"\"\n",
        "    # Special tokens, use these notations if you want\n",
        "    UNK = Language.UNK_TOKEN_IDX # Índice para tokens desconocidos (que no están en el diccionario)\n",
        "    SOS = Language.SOS_TOKEN_IDX # Índice para inicio de secuencias\n",
        "    EOS = Language.EOS_TOKEN_IDX # Índice para fin de secuencias\n",
        "\n",
        "    #Se declaran las listas de salida\n",
        "    src_sentence: List[int] = None \n",
        "    trg_sentence: List[int] = None\n",
        "\n",
        "    # Truncar secuencias. \n",
        "    # La secuencia origen se trunca a max_len palabras\n",
        "    # La secuencia target se trunca a max_len-2 palabras, eso es porque además hay que agregar <SOS> y <EOS> a la secuencia\n",
        "    truncated_src_sentence = raw_src_sentence[:max_len] if len(raw_src_sentence) > max_len else raw_src_sentence\n",
        "    truncated_tgt_sentence = raw_trg_sentence[:max_len-2] if len(raw_trg_sentence) > max_len-2 else raw_trg_sentence\n",
        "\n",
        "    # Convertir las secuencias a números. Palabras desconocidas se identifican con <UNK>\n",
        "    src_sentence = [src_word2idx[word] if word in src_word2idx.keys() else UNK for word in truncated_src_sentence]\n",
        "    trg_sentence = [trg_word2idx[word] if word in trg_word2idx.keys() else UNK for word in truncated_tgt_sentence]\n",
        "\n",
        "    # Insertar <SOS> y <EOS> a la secuencia target\n",
        "    trg_sentence.insert(0, SOS)\n",
        "    trg_sentence.append(EOS)\n",
        "\n",
        "    #Retornar las secuencias numéricas\n",
        "    return src_sentence, trg_sentence\n",
        "\n",
        "\n",
        "def bucketed_batch_indices(\n",
        "        sentence_length: List[Tuple[int, int]],\n",
        "        batch_size: int,\n",
        "        max_pad_len: int\n",
        ") -> List[List[int]]:\n",
        "    \"\"\" Function for bucketed batch indices\n",
        "    Although the loss calculation does not consider PAD tokens, it actually takes up GPU resources and degrades performance.\n",
        "    Therefore, the number of <PAD> tokens in a batch should be minimized in order to maximize GPU utilization.\n",
        "    Implement a function which groups samples into batches that satisfy the number of needed <PAD> tokens in each sentence is less than or equals to max_pad_len.\n",
        "\n",
        "    Note 1: several small batches which have less samples than batch_size are okay but should not be many. If you pass the test, it means \"okay\".\n",
        "\n",
        "    Note 2: you can directly apply this function to torch.utils.data.dataloader.DataLoader with batch_sampler argument.\n",
        "    Read the test codes if you are interested in.\n",
        "\n",
        "    Hint 1: The most easiest way for bucketing is sort-and-batch manner, but soon you will realize this doesn't work for this time.\n",
        "    The second easiest way is binning, however one-dimensional binning is not enough because there are two sentences per a sample.\n",
        "\n",
        "    Hint 2: defaultdict in collections library might be useful.\n",
        "\n",
        "    Arguments:\n",
        "    sentence_length -- list of (length of source_sentence, length of target_sentence) pairs.\n",
        "    batch_size -- batch size\n",
        "    max_pad_len -- maximum padding length. The number of needed <PAD> tokens in each sentence should not exceed this number.\n",
        "\n",
        "    return:\n",
        "    batch_indices_list -- list of indices to be a batch. Each element should contain indices of sentence_length list.\n",
        "\n",
        "    Example:\n",
        "    If sentence_length = [7, 4, 9, 2, 5, 10], batch_size = 3, and max_pad_len = 3,\n",
        "    then one of the possible batch_indices_list is [[0, 2, 5], [1, 3, 4]]\n",
        "    because [0, 2, 5] indices has simialr length as sentence_length[0] = 7, sentence_length[2] = 9, and sentence_length[5] = 10.\n",
        "    \"\"\"\n",
        "\n",
        "    batch_indices_list: List[List[int]] = None\n",
        "    sentence_length = torch.LongTensor(sentence_length)\n",
        "    min_dim0 = min(sentence_length[:,0])\n",
        "    max_dim0 = max(sentence_length[:,0])\n",
        "    min_dim1 = min(sentence_length[:,1])\n",
        "    max_dim1 = max(sentence_length[:,1])\n",
        "    dim0_bin = list(range(min_dim0, max_dim0, max_pad_len))\n",
        "    dim1_bin = list(range(min_dim1, max_dim1, max_pad_len))\n",
        "\n",
        "    bin =  []\n",
        "    for i in dim0_bin:\n",
        "        for j in dim1_bin:\n",
        "            elem = (i,j)\n",
        "            bin.append(torch.LongTensor(elem))\n",
        "    bin = torch.stack(bin)\n",
        "    diff = []\n",
        "    for i in range(len(bin)):\n",
        "        diff.append(sentence_length-bin[i])\n",
        "    diff = torch.stack(diff)\n",
        "    mask_dim1 = (diff[:, :, 0] <= 5) * (0 <= diff[:, :, 0])\n",
        "    mask_dim2 = (diff[:, :, 1] <= 5) * (0 <= diff[:, :, 1])\n",
        "    mask = mask_dim1 * mask_dim2\n",
        "    mask = mask.transpose(0,1)\n",
        "    bin_id_per_sentence = [int(mask[i].nonzero()[0]) for i in range(len(mask))]\n",
        "    sentence_idx = range(len(sentence_length))\n",
        "    dict = defaultdict(list)\n",
        "    for d0, d1 in zip(bin_id_per_sentence,sentence_idx):\n",
        "        dict[d0].append(d1)\n",
        "\n",
        "    list1 = dict.values()\n",
        "    list2 = []\n",
        "    for bin in list1:\n",
        "        n_batches = len(bin)//batch_size\n",
        "        namuji = len(bin)%batch_size\n",
        "        tmp_list = [bin[i*batch_size:(i+1)*batch_size] for i in range(n_batches)]\n",
        "        list2 += tmp_list\n",
        "        if namuji != 0:\n",
        "            last_batch = bin[n_batches*batch_size:]\n",
        "            list2.append(last_batch)\n",
        "\n",
        "    batch_indices_list = list2\n",
        "\n",
        "    # Don't forget shuffling batches because length of each batch could be biased\n",
        "    random.shuffle(batch_indices_list)\n",
        "\n",
        "    return batch_indices_list\n",
        "\n",
        "\n",
        "def collate_fn(\n",
        "    batched_samples: List[Tuple[List[int], List[int]]]\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\" Collate function\n",
        "    Because each sentence has variable length, you should collate them into one batch with <PAD> tokens.\n",
        "    Implement collate_fn function which collates source/target sentence into source/target batchs appending <PAD> tokens behind\n",
        "    Meanwhile, for the convenience of latter implementations, you should sort the sentences within a batch by its source sentence length in descending manner.\n",
        "\n",
        "    Note 1: if you are an expert on time-series data, you may know a tensor of [sequence_length, batch_size, ...] is much faster than [batch_size, sequence_length, ...].\n",
        "    However, for simple intuitive understanding, let's just use batch_first this time.\n",
        "\n",
        "    Note 2: you can directly apply this function to torch.utils.data.dataloader.DataLoader with collate_fn argument.\n",
        "    Read the test codes if you are interested in.\n",
        "\n",
        "    Hint: torch.nn.utils.rnn.pad_sequence would be useful\n",
        "\n",
        "    Arguments:\n",
        "    batched_samples -- list of (source_sentence, target_sentence) pairs. This list should be converted to a batch\n",
        "\n",
        "    Return:\n",
        "    src_sentences -- batched source sentence\n",
        "                        in shape (batch_size, max_src_sentence_length)\n",
        "    trg_sentences -- batched target sentence\n",
        "                        in shape (batch_size, max_trg_sentence_length)\n",
        "\n",
        "    \"\"\"\n",
        "    PAD = Language.PAD_TOKEN_IDX\n",
        "    batch_size = len(batched_samples)\n",
        "\n",
        "    src_sentences: torch.Tensor = None\n",
        "    trg_sentences: torch.Tensor = None\n",
        "    src = [sample[0] for sample in batched_samples]\n",
        "    tgt = [sample[1] for sample in batched_samples]\n",
        "    src_lengths = [len(sample) for sample in src]\n",
        "    tgt_lengths = [len(sample) for sample in tgt]\n",
        "    max_src_len = max(src_lengths)\n",
        "    max_tgt_len = max(tgt_lengths)\n",
        "    for i in range(len(batched_samples)):\n",
        "        src[i] = np.asarray(src[i], dtype=np.int64)\n",
        "        src[i] = np.pad(src[i], (0, max_src_len-len(src[i])), mode=\"constant\", constant_values=PAD)\n",
        "        tgt[i] = np.asarray(tgt[i], dtype=np.int64)\n",
        "        tgt[i] = np.pad(tgt[i], (0, max_tgt_len-len(tgt[i])), mode=\"constant\", constant_values=PAD)\n",
        "\n",
        "    src_sentences = torch.LongTensor(src)\n",
        "    trg_sentences = torch.LongTensor(tgt)\n",
        "\n",
        "    _, sorted_indices = torch.sort(torch.LongTensor(src_lengths), dim=0, descending=True)\n",
        "    src_sentences = src_sentences.index_select(0, sorted_indices)\n",
        "    trg_sentences = trg_sentences.index_select(0, sorted_indices)\n",
        "\n",
        "    assert src_sentences.shape[0] == batch_size and trg_sentences.shape[0] == batch_size\n",
        "    assert src_sentences.dtype == torch.long and trg_sentences.dtype == torch.long\n",
        "    return src_sentences, trg_sentences\n",
        "\n",
        "#Clase para un Idioma. Básicamente contiene los diccionarios para convertir palabras a índices y viceversa.\n",
        "class Language(Sequence[List[str]]):\n",
        "    PAD_TOKEN = '<PAD>'\n",
        "    PAD_TOKEN_IDX = 0\n",
        "    UNK_TOKEN = '<UNK>'\n",
        "    UNK_TOKEN_IDX = 1\n",
        "    SOS_TOKEN = '<SOS>'\n",
        "    SOS_TOKEN_IDX = 2\n",
        "    EOS_TOKEN = '<EOS>'\n",
        "    EOS_TOKEN_IDX = 3\n",
        "\n",
        "    def __init__(self, path: str) -> None:\n",
        "        with open(path, mode='r', encoding='utf-8') as f:\n",
        "            self._sentences: List[List[str]] = [line.split() for line in f]\n",
        "\n",
        "        self.word2idx: Dict[str, int] = None\n",
        "        self.idx2word: List[str] = None\n",
        "    \n",
        "    def build_vocab(self, min_freq: int=2) -> None:\n",
        "        SPECIAL_TOKENS: List[str] = [Language.PAD_TOKEN, Language.UNK_TOKEN, Language.SOS_TOKEN, Language.EOS_TOKEN]\n",
        "        self.idx2word = SPECIAL_TOKENS + [word for word, count in Counter(chain(*self._sentences)).items() if count >= min_freq]\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(self.idx2word)}\n",
        "    \n",
        "    def set_vocab(self, word2idx: Dict[str, int], idx2word: List[str]) -> None:\n",
        "        self.word2idx = word2idx\n",
        "        self.idx2word = idx2word\n",
        "    \n",
        "    def __getitem__(self, index: int) -> List[str]:\n",
        "        return self._sentences[index]\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self._sentences)\n",
        "\n",
        "#Clase para un dataset de traducción. Recibe dos lenguajes.\n",
        "class NmtDataset(Sequence[Tuple[List[int], List[int]]]):\n",
        "    def __init__(self, src: Language, trg: Language, max_len: int=30) -> None:\n",
        "        assert len(src) == len(trg)\n",
        "        assert src.word2idx is not None and trg.word2idx is not None\n",
        "\n",
        "        self._src = src\n",
        "        self._trg = trg\n",
        "        self._max_len = max_len\n",
        "\n",
        "    #Preprocesa dos sentencias y las retorna\n",
        "    def __getitem__(self, index: int) -> Tuple[List[str], List[str]]:\n",
        "        return preprocess(self._src[index], self._trg[index], self._src.word2idx, self._trg.word2idx, self._max_len)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._src)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import random\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "class AttentionBase(nn.Module, ABC):\n",
        "    \"\"\" Base attention class\n",
        "    You don't need to modify anything in this class\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def forward(self, encoder_hidden: torch.Tensor, encoder_mask: torch.Tensor, decoder_hidden: torch.Tensor, decoder_mask: torch.Tensor):\n",
        "        \"\"\" Abstract attention forward function\n",
        "        Your forward function should follow below arguments & returns\n",
        "        For ploting, you should return attention distribution result.\n",
        "\n",
        "        Parameters:\n",
        "        encoder_hidden -- encoder_hidden is encoder hidden state which is same with h^enc in the handout \n",
        "                            in shape (batch_size, encoder_sequence_length, hidden_dim)\n",
        "                            All values in last dimension (hidden_dim dimension) are zeros for <PAD> location.\n",
        "        encoder_mask -- encoder_mask is <PAD> mask for encoder\n",
        "                            in shape (batch_size, sequence_length) with torch.bool type\n",
        "                            True for <PAD> and False for non-<PAD>\n",
        "                            Same with (encoder_hidden == 0.).all(-1)\n",
        "        decoder_hidden -- decoder_hidden is decoder hidden state which is same with h^dec_t in the handout\n",
        "                            in shape (batch_size, hidden_dim)\n",
        "                            All values in last dimension (hidden_dim dimension) are zeros for <PAD> location.\n",
        "        decoder_mask -- decoder_mask is <PAD> mask for decoder\n",
        "                            in shape (batch_size, ) with torch.bool type\n",
        "                            True for <PAD> and False for non-<PAD>\n",
        "                            Same with (decoder_hidden == 0.).all(-1)\n",
        "\n",
        "        Return:\n",
        "        attention(context) -- attention is the result of attention which same with a_t in the handout\n",
        "                            in shape (batch_size, hidden_dim)\n",
        "        distribution(attenetion weights) -- distribution is the attention distribution same with alpha_t in the handout\n",
        "                            in shape (batch_size, encoder_sequence_length)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "#Atención basada en producto interno\n",
        "class DotAttention(AttentionBase):\n",
        "    def forward(self,\n",
        "        encoder_hidden: torch.Tensor, #Tensor con todos los estados ocultos del encoder\n",
        "        encoder_mask: torch.Tensor, #Máscara que indica en dónde hay tokens <PAD>\n",
        "        decoder_hidden: torch.Tensor,\n",
        "        decoder_mask: torch.Tensor\n",
        "    ):\n",
        "        \"\"\" Dot product attention\n",
        "        Implement dot product attention which compresses encoder_output on sequence_length axis by decoder_output\n",
        "\n",
        "        Note 1: output should attent only on non-<PAD> encoder_output words\n",
        "\n",
        "        Hint: the easiest way to make SOMETHING to zero probability is \n",
        "        setting results of SOMETHING to -infinity by \"result[SOMETHING] = float('-inf')\" and do softmax on that dimension.\n",
        "\n",
        "        Parameters / Returns: same as forward function in Attention base class\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, hidden_dim = encoder_hidden.shape\n",
        "\n",
        "        assert (encoder_mask == (encoder_hidden == 0.).all(-1)).all()\n",
        "        assert (decoder_mask == (decoder_hidden == 0.).all(-1)).all()\n",
        "\n",
        "        attention: torch.Tensor = None #Resultado de la atención\n",
        "        distribution: torch.Tensor = None #Distribución de atención\n",
        "        attention_energy = encoder_hidden.bmm(decoder_hidden.unsqueeze(-1)).squeeze(2) # Producto interno entre estados ocultos del encoder y estado oculto del decoder\n",
        "        attention_energy[encoder_mask] = float('-inf') #Aquellas posiciones con máscara se cambian por -Inf\n",
        "        distribution = torch.nn.functional.softmax(attention_energy, dim=1) #Se calcula softmax. Los valores con -Inf -> cero\n",
        "        attention = encoder_hidden.transpose(1, 2).bmm(distribution.unsqueeze(-1)).squeeze(2) #Vector de atención es la suma ponderada de estados ocultos con pesos de distribución de atención\n",
        "\n",
        "        assert attention.shape == torch.Size([batch_size, hidden_dim])\n",
        "        assert distribution.shape == torch.Size([batch_size, sequence_length])\n",
        "\n",
        "        # Don't forget setting results of decoder <PAD> token values to zeros.\n",
        "        # This would be helpful for debuging and other implementation details.\n",
        "        attention[decoder_mask, :] = 0. #Si el decoder está procesando un <PAD>, se anula la atención\n",
        "\n",
        "        return attention, distribution\n",
        "\n",
        "#Atención aditiva\n",
        "class ConcatAttention(AttentionBase):\n",
        "    def __init__(self, hidden_dim):\n",
        "        \"\"\" Concat attention initializer\n",
        "        Because there are variables in concat attention, you would need following attributes.\n",
        "        Use these attributes in forward function\n",
        "\n",
        "        Attributes:\n",
        "        W_a -- Attention weight in the handout\n",
        "                in shape (hidden_dim, 4 * hidden_dim)\n",
        "        v_a -- Attention vector in the handout\n",
        "                in shape (hidden_dim, )\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.W_a = nn.Parameter(torch.empty([hidden_dim, 4 * hidden_dim]))\n",
        "        self.v_a = nn.Parameter(torch.empty([hidden_dim]))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        nn.init.normal_(self.W_a.data)\n",
        "        nn.init.normal_(self.v_a.data)\n",
        "\n",
        "    def forward(self,\n",
        "        encoder_hidden: torch.Tensor,\n",
        "        encoder_mask: torch.Tensor,\n",
        "        decoder_hidden: torch.Tensor,\n",
        "        decoder_mask: torch.Tensor\n",
        "    ):\n",
        "        \"\"\" Concat attention forward function\n",
        "        Implement concat attention which compresses encoder_output on sequence_length axis by decoder_output\n",
        "\n",
        "        Parameters / Returns: same as forward function in Attention base class\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, hidden_dim = encoder_hidden.shape\n",
        "\n",
        "        assert (encoder_mask == (encoder_hidden == 0.).all(-1)).all()\n",
        "        assert (decoder_mask == (decoder_hidden == 0.).all(-1)).all()\n",
        "\n",
        "        attention: torch.Tensor = None\n",
        "        distribution: torch.Tensor = None\n",
        "\n",
        "        decoder_hidden_expand = decoder_hidden.unsqueeze(dim=1).expand(batch_size,sequence_length,hidden_dim)\n",
        "        concat_hidden = torch.cat((encoder_hidden, decoder_hidden_expand), dim=-1) #Se concatenan los estados ocultos\n",
        "        C = concat_hidden.matmul(self.W_a.transpose(0,1)) #Se multiplican por la matriz W\n",
        "        tanh_C = torch.tanh(C) # Se calcula Tanh\n",
        "        attention_energy = tanh_C.matmul(self.v_a) # Se multiplican por V\n",
        "        attention_energy[encoder_mask] = float('-inf') # Se anulan los datos con máscara\n",
        "        distribution = torch.nn.functional.softmax(attention_energy, dim=1) #Softmax\n",
        "        attention = encoder_hidden.transpose(1, 2).bmm(distribution.unsqueeze(-1)).squeeze(-1) #Cálculo de atención\n",
        "       \n",
        "        if attention.shape != torch.Size([batch_size, hidden_dim]):\n",
        "            assert True\n",
        "        assert distribution.shape == torch.Size([batch_size, sequence_length])\n",
        "\n",
        "        # Don't forget setting results of decoder <PAD> token values to zeros.\n",
        "        # This would be helpful for debuging and other implementation details.\n",
        "        attention[decoder_mask, :] = 0.\n",
        "\n",
        "        return attention, distribution"
      ],
      "metadata": {
        "id": "oHC6Qwo9qssX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Creamos la red neuronal para Neural Machine Translation\n",
        "# Recibe los dos idiomas, el tipo de atención\n",
        "class Seq2Seq(torch.nn.Module):\n",
        "    def __init__(self, src: Language, trg: Language, attention_type: str, embedding_dim: int=128, hidden_dim: int=64):\n",
        "        \"\"\" Seq2Seq with Attention model\n",
        "\n",
        "        Parameters:\n",
        "        src -- source language vocabs\n",
        "        trg -- target language vocabs\n",
        "        attention_type -- internal attention type: 'dot' or 'concat'\n",
        "        embeding_dim -- embedding dimension\n",
        "        hidden_dim -- hidden dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        PAD = Language.PAD_TOKEN_IDX\n",
        "        SRC_TOKEN_NUM = len(src.idx2word) # Including <PAD>\n",
        "        TRG_TOKEN_NUM = len(trg.idx2word) # Including <PAD>\n",
        "\n",
        "        ### Declare Embedding Layers\n",
        "        # Doc for Embedding Layer: https://pytorch.org/docs/stable/nn.html#embedding \n",
        "        #\n",
        "        # Note: You should set padding_idx options to embed <PAD> tokens to 0 values \n",
        "        self.src_embedding: nn.Embedding = None\n",
        "        self.trg_embedding: nn.Embedding = None\n",
        "\n",
        "        self.src_embedding = nn.Embedding(SRC_TOKEN_NUM, embedding_dim=embedding_dim, padding_idx=PAD)\n",
        "        self.trg_embedding = nn.Embedding(TRG_TOKEN_NUM, embedding_dim=embedding_dim, padding_idx=PAD)\n",
        "\n",
        "        ### Declare LSTM/LSTMCell Layers\n",
        "        # Doc for LSTM Layer: https://pytorch.org/docs/stable/nn.html#lstm\n",
        "        # Doc for LSTMCell Layer: https://pytorch.org/docs/stable/nn.html#lstmcell\n",
        "        # Explanation for bidirection RNN in torch by @ceshine_en (English): https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66\n",
        "        #\n",
        "        # Note 1: Don't forget setting batch_first option because our tensor follows [batch_size, sequence_length, ...] form.\n",
        "        # Note 2: Use one layer LSTM with bias for encoder & decoder\n",
        "        self.encoder: nn.LSTM = None\n",
        "        self.decoder: nn.LSTMCell = None\n",
        "\n",
        "        #El encoder es un LSTM de una sola capa y bidireccional\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bias=True, batch_first=True, bidirectional=True)\n",
        "        self.decoder = nn.LSTMCell(embedding_dim, hidden_dim*2, bias=True) #El decoder es una celda LSTM que va procesando la secuencia paso a paso\n",
        "        # Attention Layer\n",
        "        if attention_type == 'dot':\n",
        "            self.attention: AttentionBase = DotAttention()\n",
        "        elif attention_type == 'concat':\n",
        "            self.attention: AttentionBase = ConcatAttention(hidden_dim)\n",
        "\n",
        "        ### Declare output Layers\n",
        "        # Output layers to attain combined-output vector o_t in the handout\n",
        "        # Doc for Sequential Layer: https://pytorch.org/docs/stable/nn.html#sequential\n",
        "        # Doc for Linear Layer: https://pytorch.org/docs/stable/nn.html#linear \n",
        "        # Doc for Tanh Layer: https://pytorch.org/docs/stable/nn.html#tanh \n",
        "        #\n",
        "        # Note: Shape of combined-output o_t should be (batch_size, TRG_TOKEN_NUM - 1) because we will exclude the probability of <PAD>\n",
        "        self.output: nn.Sequential = None\n",
        "\n",
        "        #La MLP de salida termina con una capa de tantas neuronas como tokens hay en en el idioma target (básicamente es una clasificación de palabras)\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim, bias=False),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, TRG_TOKEN_NUM - 1, bias=False)\n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, src_sentences: torch.Tensor, trg_sentences: torch.Tensor, teacher_force: float=.5):\n",
        "        \"\"\" Seq2Seq forward function\n",
        "        \n",
        "        Parameters:\n",
        "        src_sentences -- batched source sentences\n",
        "                            in shape (batch_size, sentence_length)\n",
        "        trg_sentences -- batched target sentences\n",
        "                            in shape (batch_size, sentence_length)\n",
        "        teacher_force -- the probability of teacher forcing\n",
        "\n",
        "        Return:\n",
        "        loss -- average loss per a non-<PAD> word\n",
        "        \"\"\"\n",
        "        # You may use below notations\n",
        "        batch_size = src_sentences.shape[0]\n",
        "        PAD = Language.PAD_TOKEN_IDX\n",
        "        SOS = Language.SOS_TOKEN_IDX\n",
        "        EOS = Language.EOS_TOKEN_IDX\n",
        "\n",
        "        encoder_masks = src_sentences == PAD #La máscara del encoder tiene 1's en donde haya <PAD> en la secuencia de entrada\n",
        "\n",
        "        ### Encoder part (~7 lines)\n",
        "        # We strongly recommand you to use torch.nn.utils.rnn.pack_padded_sequence/pad_packed_sequence to deal with <PAD> and boost the performance.\n",
        "        # Doc for pack_padded_sequence: https://pytorch.org/docs/stable/nn.html#pack-padded-sequence\n",
        "        # Doc for pad_packed_sequence: https://pytorch.org/docs/stable/nn.html#pad-packed-sequence\n",
        "        # Because you have already sorted sentences at collate_fn, you can use pack_padded_sequence without any modification.\n",
        "        #\n",
        "        # Variable:\n",
        "        # encoder_hidden -- encoder_hidden is encoder hidden state which is same with h^enc in the handout\n",
        "        #                   in shape (batch_size, sequence_length, hidden_dim)\n",
        "        #                   All values in last dimension (hidden_dim dimension) are zeros for <PAD> location.\n",
        "        # hidden_state -- Last encoder hidden state\n",
        "        #                   in shape (batch_size, hidden_dim * 2)\n",
        "        # cell_state -- Last encoder cell state\n",
        "        #                   in shape (batch_size, hidden_dim * 2)\n",
        "        encoder_hidden: torch.Tensor = None\n",
        "        hidden_state: torch.Tensor = None\n",
        "        cell_state: torch.Tensor = None\n",
        "\n",
        "        src_embedding_seq = self.src_embedding(src_sentences) #Se le aplica el embedding a la sentencia source\n",
        "        src_lengths = (src_sentences != 0).sum(dim=1).cpu()\n",
        "        packed_seq = torch.nn.utils.rnn.pack_padded_sequence(src_embedding_seq, src_lengths, batch_first=True, enforce_sorted=True)\n",
        "\n",
        "        encoder_hidden_states, (hidden_state, cell_state) = self.encoder(packed_seq) #Se ejecuta el encoder\n",
        "        encoder_hidden, _ = torch.nn.utils.rnn.pad_packed_sequence(encoder_hidden_states,\n",
        "                                                                                 batch_first=True, padding_value=PAD)\n",
        "        encoder_hidden[encoder_masks] = 0.\n",
        "\n",
        "        # Loss initialize\n",
        "        decoder_out = trg_sentences.new_full([batch_size], fill_value=SOS) #La primera entrada del decoder es <SOS>\n",
        "        decoder_c0 = torch.cat((cell_state[0], cell_state[1]), dim=1) #Primera entrada de memoria C para el encoder\n",
        "        decoder_h0 = torch.cat((hidden_state[0], hidden_state[1]), dim=1) #Primera entrada de H para el encoder\n",
        "        sum_of_loss = 0.\n",
        "        ce_loss = nn.CrossEntropyLoss(ignore_index=PAD, reduction='sum') #CEL no se aplica sobre elementos con <PAD>\n",
        "        #Este bucle recorre cada palabra del target\n",
        "        for trg_word_idx in range(trg_sentences.shape[1] - 1):\n",
        "            # Teacher forcing: feed correct labels with a probability of teacher_force\n",
        "            #Se decide si usar la salida previa del decoder o la palabra de la secuencia target: Teacher forcing\n",
        "            decoder_input = trg_sentences[:, trg_word_idx] if torch.distributions.bernoulli.Bernoulli(teacher_force).sample() else decoder_out \n",
        "            decoder_input_embedding = self.trg_embedding(decoder_input)  # Se calcula el embedding de la palabra actual\n",
        "            decoder_h0, decoder_c0 = self.decoder(decoder_input_embedding, (decoder_h0, decoder_c0)) #La entrada a la celda LSTM es el embedding, la memoria y el estado oculto H\n",
        "            decoder_mask = trg_sentences[:, trg_word_idx + 1] == PAD #Se anulan los datos que pertenecen a <PAD>\n",
        "            decoder_h0[decoder_mask] = 0.\n",
        "\n",
        "            #Se calcula la atención entre los estados ocultos del encoder y el estado oculto actual del decoder\n",
        "            attention_output, distribution = self.attention(encoder_hidden, encoder_masks, decoder_h0, decoder_mask)\n",
        "\n",
        "            #Se concatenan el estado oculto y la salida de la atención\n",
        "            output_layer_input = torch.cat((decoder_h0, attention_output), dim=1) \n",
        "            output_logit = self.output(output_layer_input) #Se ejecuta el MLP para generar la salida de la actual palabra\n",
        "\n",
        "            # You may use below notations\n",
        "            decoder_target = trg_sentences[:, trg_word_idx+1] #Esta es la palabra que debería haber salido\n",
        "            decoder_out = torch.argmax(output_logit, dim=1) + 1 #Se calcula la predicción\n",
        "            tmp = output_logit.new_full((batch_size, 1), fill_value=float('-inf'))\n",
        "            new_logit = torch.cat((tmp, output_logit), dim=1) \n",
        "            loss = ce_loss(new_logit, decoder_target) #Se calcula el loss\n",
        "            sum_of_loss += loss #Se acumula el loss\n",
        "\n",
        "        loss = sum_of_loss\n",
        "        assert loss.shape == torch.Size([])\n",
        "        return loss / (trg_sentences[:, 1:] != PAD).sum() # Return average loss per a non-<PAD> word\n",
        "\n",
        "    def translate(self, sentence: torch.Tensor, max_len: int=30):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        sentence -- sentence to be translated\n",
        "                        in shape (sentence_length, )\n",
        "        max_len -- maximum word length of the translated stentence\n",
        "\n",
        "        Return:\n",
        "        translated -- translated sentence\n",
        "                        in shape (translated_length, ) of which translated_length <= max_len\n",
        "                        with torch.long type\n",
        "        distrubutions -- stacked attention distribution\n",
        "                        in shape (translated_length, sentence_length)\n",
        "                        This is used for ploting\n",
        "        \"\"\"\n",
        "        PAD = Language.PAD_TOKEN_IDX\n",
        "        SOS = Language.SOS_TOKEN_IDX\n",
        "        EOS = Language.EOS_TOKEN_IDX\n",
        "        sentence_length = sentence.size(0)\n",
        "\n",
        "        # Note: use argmax to get the next input word\n",
        "        translated =[]\n",
        "        distributions = []\n",
        "\n",
        "        src_embedding_seq = self.src_embedding(sentence).unsqueeze(0)\n",
        "        src_lengths = sentence_length\n",
        "\n",
        "        encoder_hidden_states, (hidden_state, cell_state) = self.encoder(src_embedding_seq)\n",
        "        encoder_hidden=encoder_hidden_states\n",
        "        \n",
        "        # Loss initialize\n",
        "        decoder_out = sentence.new_full([1], fill_value=SOS)\n",
        "        decoder_c0 = torch.cat((cell_state[0], cell_state[1]), dim=1)\n",
        "        decoder_h0 = torch.cat((hidden_state[0], hidden_state[1]), dim=1)\n",
        "        encoder_masks = (sentence == PAD).unsqueeze(0)\n",
        "        decoder_mask = False\n",
        "        for trg_word_idx in range(max_len):\n",
        "            decoder_input = decoder_out\n",
        "            decoder_input_embedding = self.trg_embedding(decoder_input)  # y_t\n",
        "            decoder_h0, decoder_c0 = self.decoder(decoder_input_embedding, (decoder_h0, decoder_c0))\n",
        "\n",
        "            decoder_h0[decoder_mask] = 0.\n",
        "            attention_output, distribution = self.attention(encoder_hidden, encoder_masks, decoder_h0, decoder_mask)\n",
        "\n",
        "            output_layer_input = torch.cat((decoder_h0, attention_output), dim=1)\n",
        "            output_logit = self.output(output_layer_input)\n",
        "\n",
        "            decoder_out = torch.argmax(output_logit, dim=1) + 1\n",
        "            translated.append(decoder_out)\n",
        "            distributions.append(distribution.squeeze(0))\n",
        "            if decoder_out == EOS:\n",
        "                break\n",
        "        translated = torch.stack(translated).squeeze(1)\n",
        "        distributions = torch.stack(distributions)\n",
        "        \n",
        "        assert translated.dim() == 1 and distributions.shape == torch.Size([translated.size(0), sentence_length])\n",
        "        return translated, distributions"
      ],
      "metadata": {
        "id": "GFny5RRVpZOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import datetime\n",
        "import torch.utils\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "attention_type = 'concat' # 'dot' or 'concat'\n",
        "embedding_dim = 128\n",
        "hidden_dim = 64\n",
        "bucketing = True\n",
        "\n",
        "def plot_attention(attention: torch.Tensor, trg_text: List[str], src_text: List[str], name: str):\n",
        "    assert attention.shape[0] == len(trg_text) and attention.shape[1] == len(src_text)\n",
        "    _, ax = plt.subplots()\n",
        "    _ = ax.pcolor(attention)\n",
        "\n",
        "    ax.set_xticks([tick + .5 for tick in range(len(src_text))], minor=False)\n",
        "    ax.set_yticks([tick + .5 for tick in range(len(trg_text))], minor=False)\n",
        "\n",
        "    ax.invert_yaxis()\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_xticklabels(src_text, rotation=90, minor=False)\n",
        "    ax.set_yticklabels(trg_text, minor=False)\n",
        "    plt.savefig('attention_' + name + '.png')\n",
        "\n",
        "def load_model():\n",
        "    french = Language(path='data/train.fr.txt')\n",
        "    english = Language(path='data/train.en.txt')\n",
        "    french.build_vocab()\n",
        "    english.build_vocab()\n",
        "    dataset = NmtDataset(src=french, trg=english)\n",
        "    model = Seq2Seq(french, english, attention_type=attention_type,\n",
        "                    embedding_dim=embedding_dim, hidden_dim=hidden_dim).to(device)\n",
        "    model.load_state_dict(torch.load('/home/admin/projects/ai605/assn2/seq2seq_concat.pth'))\n",
        "    model.eval()\n",
        "\n",
        "def train():\n",
        "    max_epoch = 200\n",
        "    batch_size = 256\n",
        "\n",
        "    french = Language(path='data/train.fr.txt')\n",
        "    english = Language(path='data/train.en.txt')\n",
        "    french.build_vocab()\n",
        "    english.build_vocab()\n",
        "    dataset = NmtDataset(src=french, trg=english)\n",
        "\n",
        "    max_pad_len = 5\n",
        "    sentence_length = list(map(lambda pair: (len(pair[0]), len(pair[1])), dataset))\n",
        "    batch_sampler = bucketed_batch_indices(sentence_length, batch_size=batch_size, max_pad_len=max_pad_len) if bucketing else None\n",
        "\n",
        "    model = Seq2Seq(french, english, attention_type=attention_type, embedding_dim=embedding_dim, hidden_dim=hidden_dim).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, collate_fn=collate_fn, num_workers=2, batch_size=1 if bucketing else batch_size, batch_sampler=batch_sampler, shuffle=not bucketing)\n",
        "    \n",
        "    loss_log = tqdm(total=0, bar_format='{desc}', position=0)\n",
        "    for epoch in trange(max_epoch, desc=\"Epoch\", position=0):\n",
        "        for src_sentence, trg_sentence in tqdm(dataloader, desc=\"Iteration\", position=0):\n",
        "            optimizer.zero_grad()\n",
        "            src_sentence, trg_sentence = src_sentence.to(device), trg_sentence.to(device)\n",
        "            loss = model(src_sentence, trg_sentence, teacher_force=0.5)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            des = 'Loss per a non-<PAD> Word: {:06.4f}'.format(loss.cpu())\n",
        "            loss_log.set_description_str(des)\n",
        "    \n",
        "    torch.save(model.state_dict(), \"seq2seq_\" + attention_type + \".pth\")\n",
        "\n",
        "def translate():\n",
        "    SOS = Language.SOS_TOKEN_IDX\n",
        "    EOS = Language.EOS_TOKEN_IDX\n",
        "\n",
        "    french_train = Language(path='data/train.fr.txt')\n",
        "    english_train = Language(path='data/train.en.txt')\n",
        "    french_train.build_vocab()\n",
        "    english_train.build_vocab()\n",
        "    model = Seq2Seq(french_train, english_train, attention_type=attention_type,\n",
        "                    embedding_dim=embedding_dim, hidden_dim=hidden_dim).to(device)\n",
        "    model.load_state_dict(torch.load(\"seq2seq_\" + attention_type + \".pth\", map_location=device))\n",
        "\n",
        "    french_test = Language(path='data/test.fr.txt')\n",
        "    english_test = Language(path='data/test.en.txt')\n",
        "    french_test.set_vocab(french_train.word2idx, french_train.idx2word)\n",
        "    english_test.set_vocab(english_train.word2idx, english_train.idx2word)\n",
        "    dataset = NmtDataset(src=french_test, trg=english_test)\n",
        "    \n",
        "    samples = [dataset[16][0], dataset[1][0], dataset[2][0]] # You may choose your own samples to plot\n",
        "\n",
        "    for i, french in enumerate(samples):\n",
        "        translated, attention = model.translate(torch.Tensor(french).to(dtype=torch.long, device=device))\n",
        "        source_text = [french_train.idx2word[idx] for idx in french]\n",
        "        translated_text = [english_train.idx2word[idx] for idx in translated]\n",
        "        plot_attention(attention.cpu().detach(), translated_text, source_text, name=attention_type + '_' + str(i))\n",
        "\n",
        "    f = open('translated.txt', mode='w', encoding='utf-8')\n",
        "    f_bleu = open('pred.en.txt', mode='w', encoding='utf-8')\n",
        "    for french, english in tqdm(dataset, desc='Translated'):\n",
        "        translated, attention = model.translate(torch.Tensor(french).to(dtype=torch.long, device=device))\n",
        "        source_text = [french_train.idx2word[idx] for idx in french]\n",
        "        target_text = [english_train.idx2word[idx] for idx in english if idx != SOS and idx != EOS]\n",
        "        translated_text = [english_train.idx2word[idx] for idx in translated if idx != EOS]\n",
        "\n",
        "        f.write('French    : ' + ' '.join(source_text) + '\\n')\n",
        "        f.write('English   : ' + ' '.join(target_text) + '\\n')\n",
        "        f.write('Translated: ' + ' '.join(translated_text) + '\\n\\n')\n",
        "        f_bleu.write(' '.join(translated_text) + '\\n')\n",
        "    f.close()\n",
        "    f_bleu.close()"
      ],
      "metadata": {
        "id": "ob27apjqpiZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(precision=8)\n",
        "random.seed(4321)\n",
        "torch.manual_seed(4321)\n",
        "print(datetime.datetime.now())\n",
        "train()\n",
        "print(datetime.datetime.now())\n",
        "translate()"
      ],
      "metadata": {
        "id": "PxZyCavqpkco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mVtDwuk8smX1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}